{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfeb85a1",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ff0c81",
   "metadata": {},
   "source": [
    "## Table of content \n",
    "\n",
    "### 1 Introduction to Classification\n",
    "\n",
    "* What is classification?\n",
    "* Types of classification problems\n",
    "* Real-world examples of classification tasks\n",
    "\n",
    "### 2 Getting Started with scikit-learn\n",
    "\n",
    "* Installing scikit-learn\n",
    "* Understanding the scikit-learn API\n",
    "* Loading datasets and preprocessing\n",
    "\n",
    "### 3 Supervised Learning: Classification Techniques\n",
    "\n",
    "* a. Logistic Regression\n",
    "* b. K-Nearest Neighbors (KNN)\n",
    "* c. Support Vector Machines (SVM)\n",
    "* d. Decision Trees\n",
    "* e. Random Forest\n",
    "* f. Naive Bayes\n",
    "\n",
    "### 4 Model Evaluation and Selection\n",
    "\n",
    "* Train-test split\n",
    "* Cross-validation\n",
    "* Performance metrics (accuracy, precision, recall, F1-score, ROC-AUC)\n",
    "* Hyperparameter tuning (Grid search, Random search)\n",
    "* Model selection and comparison\n",
    "\n",
    "### 5 Advanced Classification Techniques\n",
    "* a. Ensemble Methods\n",
    "* b. Imbalanced Classification\n",
    "\n",
    "### 6 Feature Selection and Dimensionality Reduction\n",
    "\n",
    "* Variance threshold\n",
    "* Recursive feature elimination (RFE)\n",
    "* Principal component analysis (PCA)\n",
    "* Linear discriminant analysis (LDA)\n",
    "\n",
    "### 7  Practical Project\n",
    "\n",
    "* Choosing a classification dataset\n",
    "* Data preprocessing and exploration\n",
    "* Model selection, training, and evaluation\n",
    "* Hyperparameter tuning and model optimization\n",
    "* Presenting the final results  \n",
    "\n",
    "### 8 Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f05994f",
   "metadata": {},
   "source": [
    "## 1 Introduction to Classification\n",
    "\n",
    "### What is classification?\n",
    "\n",
    "Classification is a type of supervised machine learning task in which the goal is to assign objects or instances to predefined categories or classes. In supervised learning, the model learns from a dataset that contains input-output pairs, where the output (or target) is a discrete value representing the class label. Classification models can be used to predict the class of an object based on its input features.\n",
    "\n",
    "### Types of classification problems:\n",
    "\n",
    "There are two main types of classification problems:\n",
    "    \n",
    "    \n",
    "\n",
    "* a. Binary Classification: In binary classification, there are only two possible classes. The model is trained to distinguish between these two classes. For example, classifying emails as spam or not spam.\n",
    "\n",
    "* b. Multiclass Classification: In multiclass classification, there are more than two possible classes. The model is trained to classify instances into one of the multiple classes. For example, classifying handwritten digits into one of the ten classes (0 to 9)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0efb6445",
   "metadata": {},
   "source": [
    "In some cases, you might also encounter multilabel classification problems, where each instance can be assigned to multiple classes simultaneously. For example, classifying a text document into multiple topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d767a8",
   "metadata": {},
   "source": [
    "### Real-world examples of classification tasks:\n",
    "\n",
    "\n",
    "Here are some real-world examples of classification tasks:\n",
    "\n",
    "* a. Email spam detection: Identifying whether an email is spam or not based on its content and other features.\n",
    "\n",
    "* b. Medical diagnosis: Predicting the presence or absence of a disease based on patient data (e.g., symptoms, lab results).\n",
    "\n",
    "* c. Sentiment analysis: Determining the sentiment (positive, negative, or neutral) of a given text or document.\n",
    "\n",
    "* d. Handwritten digit recognition: Identifying the digit (0 to 9) represented by a handwritten image.\n",
    "\n",
    "* e. Fraud detection: Detecting fraudulent transactions in a financial dataset based on transaction data and user behavior.\n",
    "\n",
    "* f. Image classification: Categorizing images into predefined classes, such as animals, objects, or scenes.\n",
    "\n",
    "* g. Customer segmentation: Classifying customers into groups based on their behavior or preferences for targeted marketing.\n",
    "\n",
    "These are just a few examples; classification problems are widespread across various domains and industries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b825ff93",
   "metadata": {},
   "source": [
    "## 2 Getting Started with scikit-learn\n",
    "\n",
    "### Installing scikit-learn:\n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning that provides simple and efficient tools for data mining and data analysis. To install scikit-learn, you can use the following command with pip:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e95e1ecc",
   "metadata": {},
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6b901eb",
   "metadata": {},
   "source": [
    "Or, if you are using Anaconda, use the following command with conda:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28852e70",
   "metadata": {},
   "source": [
    "conda install scikit-learn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6504cc22",
   "metadata": {},
   "source": [
    "### Understanding the scikit-learn API:\n",
    "\n",
    "\n",
    "Scikit-learn follows a consistent and easy-to-understand API structure. The main components of the API include:\n",
    "\n",
    "* a. **Estimators**: The base object for all algorithms in scikit-learn. Estimators can be classifiers, regressors, or transformers. They expose a fit method for learning a model from the training data.\n",
    "\n",
    "* b. **Transformers**: These are special types of estimators that can transform the input data. They expose a transform method for converting the input data, and a fit_transform method for fitting and transforming the data in a single step.\n",
    "\n",
    "* c. **Predictors**: These are estimators that can make predictions given the input data. They expose a predict method for generating predictions and a score method for evaluating the quality of the predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fc5c7d3",
   "metadata": {},
   "source": [
    "### Loading datasets and preprocessing:\n",
    "\n",
    "Scikit-learn provides various utilities for loading datasets and preprocessing the data. Some common tasks include:\n",
    "\n",
    "* a. **Loading datasets**: Scikit-learn comes with several built-in datasets (e.g., iris, digits, breast cancer) that can be loaded using the datasets module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "#iris.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00bf75ac",
   "metadata": {},
   "source": [
    "* b. **Data splitting**: Split the data into training and testing sets using the train_test_split function from the model_selection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42, stratify=iris.target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97625ec8",
   "metadata": {},
   "source": [
    "* c. **Feature scaling**Standardize or normalize the data using transformers like StandardScaler or MinMaxScaler from the preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f3db1ae",
   "metadata": {},
   "source": [
    "* **Handling missing values** : Impute missing values using transformers like SimpleImputer from the impute module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a092f069",
   "metadata": {},
   "source": [
    "By understanding the scikit-learn API and its utilities, you can load, preprocess, and prepare your data for various classification tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4eb6897b",
   "metadata": {},
   "source": [
    "# 3 Supervised Learning: Classification Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc3676c4",
   "metadata": {},
   "source": [
    "### a. Logistic Regression\n",
    "\n",
    "#### Understanding logistic regression: \n",
    "\n",
    "Logistic regression is a linear model used for binary classification tasks. It estimates the probability of an instance belonging to a class using the logistic function (sigmoid function). The model is trained to find the best-fitting decision boundary that separates the two classes.\n",
    "\n",
    "#### Implementing logistic regression with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "predictions = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8db6bb5",
   "metadata": {},
   "source": [
    "### b. K-Nearest Neighbors (KNN)\n",
    "\n",
    "#### Understanding KNN: \n",
    "\n",
    "K-Nearest Neighbors is a non-parametric, instance-based learning algorithm used for classification tasks. Given a new instance, KNN finds the k nearest training instances in the feature space and assigns the majority class label among these neighbors.\n",
    "\n",
    "#### Implementing KNN with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac247c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "predictions = knn.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "900b3b7d",
   "metadata": {},
   "source": [
    "### c. Support Vector Machines (SVM)\n",
    "\n",
    "#### Understanding SVM:\n",
    "Support Vector Machines is a powerful classification algorithm that can be used for linear or non-linear classification tasks. The main idea of SVM is to find the hyperplane that best separates the classes with the maximum margin, which is the distance between the hyperplane and the nearest instances from each class (support vectors).\n",
    "\n",
    "#### Implementing SVM with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "predictions = svm.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd83a816",
   "metadata": {},
   "source": [
    "### d. Decision Trees\n",
    "\n",
    "#### Understanding decision trees: \n",
    "Decision trees are a type of flowchart-like structure used for classification tasks. The tree consists of nodes, which represent features or decisions, and branches, which represent the outcome of a decision. The model is trained to recursively split the data based on the feature that provides the best separation of the classes.\n",
    "\n",
    "#### Implementing decision trees with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f10d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dtree.fit(X_train_scaled, y_train)\n",
    "predictions = dtree.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28d0694f",
   "metadata": {},
   "source": [
    "### e. Random Forest\n",
    "\n",
    "#### Understanding random forest: \n",
    "\n",
    "Random Forest is an ensemble learning method that constructs multiple decision trees and combines their predictions through majority voting. It improves the performance and generalization of a single decision tree by reducing overfitting and adding randomness to the model.\n",
    "\n",
    "#### Implementing random forest with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "predictions = rf.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eca412e",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "#### Understanding naive bayes: \n",
    "\n",
    "Naive Bayes is a family of probabilistic classification algorithms based on Bayes' theorem, with an assumption of independence between features. Common types include Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. It is particularly useful for large-scale text classification tasks.\n",
    "\n",
    "#### Implementing naive bayes with scikit-learn:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "predictions = gnb.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54be9a21",
   "metadata": {},
   "source": [
    "## 4 Model Evaluation and Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3d7c51",
   "metadata": {},
   "source": [
    "### Train-test split:\n",
    "The train-test split is a technique used to divide the dataset into two parts, one for training the model and the other for testing the model's performance. This helps to evaluate the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25766dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a57afd",
   "metadata": {},
   "source": [
    "### Cross-validation:\n",
    "Cross-validation is a more robust technique for evaluating the model's performance by dividing the dataset into multiple folds. The model is trained and tested multiple times, using different combinations of training and testing folds. The most common method is k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dc7893d",
   "metadata": {},
   "source": [
    "### Performance metrics:\n",
    "\n",
    "Various performance metrics can be used to evaluate the quality of a classification model. Some common metrics include:\n",
    "\n",
    "* **Accuracy**: The proportion of correct predictions among the total number of instances.\n",
    "* **Precision**: The proportion of true positives among the instances predicted as positive.\n",
    "* **Recall (Sensitivity)**: The proportion of true positives among the instances that are actually positive.\n",
    "* **F1-score:** The harmonic mean of precision and recall.\n",
    "* **ROC-AUC:** The area under the receiver operating characteristic (ROC) curve, which plots the true positive rate against the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a849f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f552dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        15\\n           1       0.82      0.93      0.87        15\\n           2       0.92      0.80      0.86        15\\n\\n    accuracy                           0.91        45\\n   macro avg       0.92      0.91      0.91        45\\nweighted avg       0.92      0.91      0.91        45\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions)\n",
    "report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91185506",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning:\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal values for the hyperparameters of a model. Two popular methods for hyperparameter tuning are grid search and random search.\n",
    "\n",
    "#### Grid search: \n",
    "Exhaustively tries all possible combinations of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e176eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17b579c0",
   "metadata": {},
   "source": [
    "#### Random search: \n",
    "\n",
    "Samples a random combination of hyperparameter values within specified ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48003e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dist = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "random_search = RandomizedSearchCV(SVC(), param_dist, n_iter=10, cv=5)\n",
    "random_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb5edcc8",
   "metadata": {},
   "source": [
    "### Model selection and comparison:\n",
    "\n",
    "After evaluating the performance of different models and tuning their hyperparameters, you can compare the models and select the one that performs best on the chosen metrics. This will help you choose the most suitable model for your classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab762d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing the accuracy of two models\n",
    "accuracy_logreg = accuracy_score(y_test, predictions_logreg)\n",
    "accuracy_knn = accuracy_score(y_test, predictions_knn)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"K-Nearest Neighbors Accuracy:\", accuracy_knn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42c89f4c",
   "metadata": {},
   "source": [
    "### 5 Advanced Classification Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cd5164d",
   "metadata": {},
   "source": [
    "### a. Ensemble Methods\n",
    "\n",
    "Ensemble methods are techniques that combine the predictions of multiple base models to improve the overall performance and generalization of the final model. There are three main types of ensemble methods:\n",
    "\n",
    "#### Bagging: \n",
    "Bagging (Bootstrap Aggregating) is an ensemble method that trains multiple base models, typically decision trees, using different subsets of the training data, sampled with replacement. The final prediction is obtained by averaging (for regression) or majority voting (for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging.fit(X_train_scaled, y_train)\n",
    "predictions = bagging.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d16f84",
   "metadata": {},
   "source": [
    "#### Boosting: \n",
    "\n",
    "Boosting algorithms train a sequence of weak models, typically decision trees, with each model learning from the errors of its predecessor. Popular boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "##### AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04858d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=42)\n",
    "adaboost.fit(X_train_scaled, y_train)\n",
    "predictions = adaboost.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfef6043",
   "metadata": {},
   "source": [
    "##### Gradient Boosting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gboost.fit(X_train_scaled, y_train)\n",
    "predictions = gboost.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd46e71",
   "metadata": {},
   "source": [
    "#### Stacking: \n",
    "Stacking combines the predictions of multiple base models using a meta-model, which is trained on the output of the base models. This allows the meta-model to learn how to best combine the predictions of the base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce790a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "base_estimators = [('logreg', LogisticRegression()), ('knn', KNeighborsClassifier()), ('svm', SVC())]\n",
    "stacking = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "stacking.fit(X_train_scaled, y_train)\n",
    "predictions = stacking.predict(X_test_scaled)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8d587b6",
   "metadata": {},
   "source": [
    "### b. Imbalanced Classification\n",
    "Imbalanced classification deals with datasets where one class is significantly under-represented compared to the other classes. This can lead to biased models that perform poorly on the minority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bee03aa6",
   "metadata": {},
   "source": [
    "#### Understanding imbalanced datasets: \n",
    "\n",
    "Imbalanced datasets can occur in real-world problems like fraud detection, medical diagnosis, and rare event prediction. The imbalance can lead to a higher misclassification rate for the minority class, as the model is biased towards the majority class.\n",
    "\n",
    "#### Resampling techniques: \n",
    "\n",
    "Resampling techniques can be used to balance the class distribution by either oversampling the minority class or undersampling the majority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "992165f6",
   "metadata": {},
   "source": [
    "* **Oversampling**: Randomly replicating instances from the minority class to increase its representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy='minority')\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "303aa3d6",
   "metadata": {},
   "source": [
    "* **Undersampling**: Randomly removing instances from the majority class to decrease its representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec102c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9786977d",
   "metadata": {},
   "source": [
    "#### Evaluation metrics for imbalanced datasets: \n",
    "\n",
    "In imbalanced datasets, accuracy is not a suitable metric, as it can be misleading due to the bias towards the majority class. Instead, other metrics like precision, recall, F1-score, and the area under the precision-recall curve (PR-AUC) should be used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa0a9f19",
   "metadata": {},
   "source": [
    "Precision-Recall Curve and PR-AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "pr_auc = auc(recall, precision)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f30f63a",
   "metadata": {},
   "source": [
    "By using advanced classification techniques like ensemble methods and addressing imbalanced datasets with resampling techniques, you can improve the performance and generalization of your classification models. Additionally, using appropriate evaluation metrics will help you better assess and compare models on imbalanced datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c0c129",
   "metadata": {},
   "source": [
    "## 6 Feature Selection and Dimensionality Reduction\n",
    "\n",
    "Feature selection and dimensionality reduction techniques help to select the most important features and reduce the number of features used in a model. This can lead to simpler models, lower computational cost, and improved performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be553045",
   "metadata": {},
   "source": [
    "### Variance threshold:\n",
    "Variance threshold is a simple feature selection technique that removes features with variance below a certain threshold. This is based on the assumption that features with low variance don't contribute much to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "X_high_variance = selector.fit_transform(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62073898",
   "metadata": {},
   "source": [
    "### Recursive feature elimination (RFE):\n",
    "\n",
    "RFE is a feature selection technique that iteratively removes features based on their importance scores, which can be calculated using the coefficients or feature importances of a model. RFE ranks the features and selects the best subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c932935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "X_rfe = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bedc8b4",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA):\n",
    "PCA is a dimensionality reduction technique that projects the original data into a lower-dimensional space while preserving the maximum variance. It can help to reduce noise and improve the performance of models, especially when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "X_pca = pca.fit_transform(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c334b483",
   "metadata": {},
   "source": [
    "### Linear discriminant analysis (LDA):\n",
    "\n",
    "LDA is a supervised dimensionality reduction technique that seeks to find a linear combination of features that maximizes the separation between classes. LDA can improve the performance of classification models by reducing the number of features while preserving class separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)  # Reduce to 1 linear discriminant\n",
    "X_lda = lda.fit_transform(X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd26ce9",
   "metadata": {},
   "source": [
    "By applying feature selection and dimensionality reduction techniques, you can simplify your classification models and potentially improve their performance. These techniques can also help to reduce the computational cost and training time of your models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e126cb",
   "metadata": {},
   "source": [
    "## 7 Practical Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21fb76f3",
   "metadata": {},
   "source": [
    "In this practical project, we will go through the process of choosing a real-world classification dataset, preprocessing and exploring the data, selecting, training, and evaluating models, tuning hyperparameters, and presenting the final results.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24e129b9",
   "metadata": {},
   "source": [
    "### 1 Choosing a classification dataset:\n",
    "Find a suitable classification dataset for your project. Examples include the Iris dataset, the Breast Cancer Wisconsin dataset, or the Wine Quality dataset. You can also explore public datasets available on platforms like Kaggle or UCI Machine Learning Repository.\n",
    "\n",
    "### 2 Data preprocessing and exploration:\n",
    "Load the dataset, clean the data if necessary, and perform exploratory data analysis to understand the data's characteristics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c84fa1f",
   "metadata": {},
   "source": [
    "In this example, I'll use the Wine Quality dataset from the UCI Machine Learning Repository. Here's the code to load the data and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Basic statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Visualize feature distributions and relationships\n",
    "sns.pairplot(data, hue='quality', corner=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa68d586",
   "metadata": {},
   "source": [
    "### 3 Model selection, training, and evaluation:\n",
    "\n",
    "Split the data into training and testing sets, train different classification models, and evaluate their performance using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e14197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Preprocessing\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training and evaluating models\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_preds = logreg.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc_preds = svc.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "logreg_metrics = evaluate(y_test, logreg_preds)\n",
    "rf_metrics = evaluate(y_test, rf_preds)\n",
    "svc_metrics = evaluate(y_test, svc_preds)\n",
    "\n",
    "print(\"Logistic Regression Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*logreg_metrics))\n",
    "print(\"Random Forest Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*rf_metrics))\n",
    "print(\"SVM Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*svc_metrics))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c671b275",
   "metadata": {},
   "source": [
    "This code loads the Wine Quality dataset, preprocesses it, splits it into training and testing sets, trains three classification models, and evaluates their performance using accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "246abbcc",
   "metadata": {},
   "source": [
    "### 4 Hyperparameter tuning and model optimization:\n",
    "    \n",
    "Optimize the models' performance by tuning their hyperparameters using techniques like grid search or random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Preprocessing\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training and evaluating the baseline model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "rf_metrics = evaluate(y_test, rf_preds)\n",
    "print(\"Baseline Random Forest Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*rf_metrics))\n",
    "\n",
    "# Hyperparameter tuning using Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Training and evaluating the optimized model\n",
    "rf_optimized = RandomForestClassifier(**best_params)\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "rf_optimized_preds = rf_optimized.predict(X_test)\n",
    "\n",
    "rf_optimized_metrics = evaluate(y_test, rf_optimized_preds)\n",
    "print(\"Optimized Random Forest Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*rf_optimized_metrics))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b4af1f",
   "metadata": {},
   "source": [
    "This code loads the Wine Quality dataset, preprocesses it, splits it into training and testing sets, trains a baseline Random Forest model, and evaluates its performance. Then, it performs hyperparameter tuning using Grid Search and retrains the optimized model, evaluating its performance to compare with the baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6280de7f",
   "metadata": {},
   "source": [
    "### 5 Presenting the final results:\n",
    "After optimizing the models, select the best model based on the chosen evaluation metrics, and present the final results, including the model's performance on the test dataset, feature importances or coefficients, and any insights derived from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b662c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# Preprocessing\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter tuning using Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Training and evaluating the optimized model\n",
    "rf_optimized = RandomForestClassifier(**best_params)\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "rf_optimized_preds = rf_optimized.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "def evaluate(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "rf_optimized_metrics = evaluate(y_test, rf_optimized_preds)\n",
    "print(\"Optimized Random Forest Metrics - Accuracy: {}, Precision: {}, Recall: {}, F1-score: {}\".format(*rf_optimized_metrics))\n",
    "\n",
    "# Identifying important features\n",
    "important_features = pd.Series(rf_optimized.feature_importances_, index=X.columns)\n",
    "important_features = important_features.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nImportant Features:\")\n",
    "print(important_features)\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\nBased on the evaluation metrics, the Optimized Random Forest model is the best-performing model.\")\n",
    "print(\"The top features contributing to wine quality prediction are:\")\n",
    "print(important_features.head(5))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f41a97fb",
   "metadata": {},
   "source": [
    "This code loads the Wine Quality dataset, preprocesses it, splits it into training and testing sets, performs hyperparameter tuning using Grid Search, and trains the optimized Random Forest model. It then evaluates the model's performance on the test dataset, identifies the important features, and presents the final results.\n",
    "\n",
    "In the conclusion, we report the best model based on the chosen evaluation metrics and list the top features contributing to wine quality prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b6e11e4",
   "metadata": {},
   "source": [
    "## 8 Assignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29cd3227",
   "metadata": {},
   "source": [
    "## Assignment: Predicting Customer Churn\n",
    "\n",
    "### Objective: \n",
    "\n",
    "The goal of this assignment is to build a classification model to predict whether a customer will churn (stop using a service) based on their features and interactions with the service.\n",
    "\n",
    "### Dataset: \n",
    "The Telco Customer Churn dataset, available on Kaggle, contains information about a fictional telecommunication company's customers and whether they have churned. You can download the dataset here.\n",
    " https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "* Load and explore the dataset: Analyze the distribution of features, check for missing values, and visualize relationships between features and the target variable (churn).\n",
    "\n",
    "    \n",
    "    \n",
    "* Preprocess the data: Handle missing values, convert categorical variables to numeric, and normalize/standardize the features if necessary.\n",
    "\n",
    "    \n",
    "    \n",
    "* Split the dataset: Divide the dataset into training and testing sets.\n",
    "\n",
    "    \n",
    "    \n",
    "* Train classification models: Train various classification models (e.g., logistic regression, KNN, SVM, decision tree, random forest, etc.) on the training dataset.\n",
    "\n",
    "    \n",
    "    \n",
    "* Evaluate the models: Assess the performance of the models using appropriate metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
    "\n",
    "    \n",
    "    \n",
    "* Optimize the models: Perform hyperparameter tuning using techniques like grid search or random search to improve the performance of the models.\n",
    "\n",
    "    \n",
    "    \n",
    "* Feature selection and dimensionality reduction: Apply feature selection techniques such as RFE, variance threshold, or dimensionality reduction methods like PCA and LDA to reduce the number of features and potentially improve model performance.\n",
    "\n",
    "    \n",
    "    \n",
    "* Select the best model: Choose the best-performing model based on the evaluation metrics.\n",
    "\n",
    "    \n",
    "    \n",
    "* Interpret the results: Discuss the performance of the chosen model, the importance of different features, and any insights gained from the analysis.\n",
    "\n",
    "    \n",
    "    \n",
    "* Conclusion: Summarize the findings, mention any limitations of the project, and suggest possible improvements or future work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a62b5782",
   "metadata": {},
   "source": [
    "# solution for the Customer Churn assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591012a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data/Telco-Customer-Churn.csv')\n",
    "\n",
    "# Preprocessing\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "data['TotalCharges'].fillna(data['TotalCharges'].mean(), inplace=True)\n",
    "\n",
    "for column in data.select_dtypes(include=['object']):\n",
    "    if column != 'customerID':\n",
    "        data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "# Splitting dataset\n",
    "X = data.drop(['customerID', 'Churn'], axis=1)\n",
    "y = data['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training and evaluating models\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_preds = logreg.predict(X_test)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_preds)\n",
    "logreg_precision = precision_score(y_test, logreg_preds)\n",
    "logreg_recall = recall_score(y_test, logreg_preds)\n",
    "logreg_f1 = f1_score(y_test, logreg_preds)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "rf_precision = precision_score(y_test, rf_preds)\n",
    "rf_recall = recall_score(y_test, rf_preds)\n",
    "rf_f1 = f1_score(y_test, rf_preds)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {logreg_accuracy}, Precision: {logreg_precision}, Recall: {logreg_recall}, F1: {logreg_f1}\")\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy}, Precision: {rf_precision}, Recall: {rf_recall}, F1: {rf_f1}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Feature selection\n",
    "rfe = RFE(RandomForestClassifier(**best_params), n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "rf_optimized = RandomForestClassifier(**best_params)\n",
    "rf_optimized.fit(X_train_rfe, y_train)\n",
    "rf_optimized_preds = rf_optimized.predict(X_test_rfe)\n",
    "\n",
    "rf_optimized_accuracy = accuracy_score(y_test, rf_optimized_preds)\n",
    "rf_optimized_precision = precision_score(y_test, rf_optimized_preds)\n",
    "rf_optimized_recall = recall_score(y_test, rf_optimized_preds)\n",
    "rf_optimized_f1 = f1_score(y_test, rf_optimized_preds)\n",
    "\n",
    "print(f\"Optimized Random Forest - Accuracy: {rf_optimized_accuracy}, Precision: {rf_optimized_precision}, Recall: {rf_optimized_recall}, F1: {rf_optimized_f1}\")\n",
    "\n",
    "\n",
    "#Identifying important features\n",
    "important_features = pd.Series(rf_optimized.feature_importances_, index=X.columns[rfe.support_])\n",
    "important_features = important_features.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nImportant Features:\")\n",
    "print(important_features)\n",
    "\n",
    "\n",
    "#Conclusion\n",
    "print(\"\\nBased on the evaluation metrics, the Optimized Random Forest model is the best-performing model.\")\n",
    "print(\"The top features contributing to customer churn prediction are:\")\n",
    "print(important_features.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb33b6ea",
   "metadata": {},
   "source": [
    "This code provides a compact solution to the Customer Churn assignment. It loads the data, preprocesses it, trains different models, optimizes the hyperparameters, selects the most important features, and presents the results.\n",
    "\n",
    "Remember that in a real-world scenario, it's crucial to explore the data and models in more detail and interpret the results accordingly. Additionally, it is recommended to try other advanced classification techniques or address class imbalance issues if applicable to your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d1d735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
